{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/redes_neurais_esp/blob/main/RNN/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrZ0aln9hLZr"
      },
      "source": [
        "#RNN (Recurrent Neural Network)\n",
        "Notebook baseado em: https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/.\n",
        "\n",
        "As redes neurais tradicionais de feed-forward recebem uma quantidade fixa de dados de entrada ao mesmo tempo e produzem uma quantidade fixa de saída a cada vez. Por outro lado, os RNNs não consomem todos os dados de entrada de uma vez. Em vez disso, eles os pegam um de cada vez e em sequência. Em cada etapa, o RNN faz uma série de cálculos antes de produzir uma saída. A saída, conhecida como estado oculto, é então combinada com a próxima entrada na sequência para produzir outra saída. Este processo continua até que o modelo seja programado para terminar ou a sequência de entrada termine.\n",
        "\n",
        "![](https://raw.githubusercontent.com/valmirf/redes_neurais/master/PyTorch/FIG/rnn.gif?token=AFFPDEK6H5UKFZOLMD2S7WS7YBB7I)\n",
        "\n",
        "\n",
        "Os cálculos em cada etapa de tempo consideram o contexto das etapas de tempo anteriores na forma do estado oculto. Ser capaz de usar essas informações contextuais de entradas anteriores é a essência do sucesso dos RNNs em problemas sequenciais.\n",
        "\n",
        "Embora possa parecer que uma célula RNN diferente está sendo usada a cada passo de tempo nos gráficos, o princípio subjacente das Redes Neurais Recorrentes é que a célula RNN é na verdade exatamente a mesma e reutilizada por completo.\n",
        "\n",
        "É possível controlar quando a RNN vai gerar uma saída. Podemos gerar uma saída a cada apresentação de entrada, ou gerar uma saída apenas após apresentar uma entrada completa ou do modo que quisermos. Na Figura abaixo, pode-se ver uma RNN com todas as entradas e saídas. Se quisermos uma saída apenas no final, é só ignorar as entradas inermediárias e gerar apenas a saída final (_Output X_). Como esta saída final já passou por cálculos em todas as células anteriores, o contexto de todas as entradas anteriores foi capturado. Isso significa que o resultado final é realmente dependente de todos os cálculos e entradas anteriores.\n",
        "\n",
        "![](https://raw.githubusercontent.com/valmirf/redes_neurais/master/PyTorch/FIG/rnn2.jpg?token=AFFPDEOMVP2ISECTDYTW7RS7YBCC2)\n",
        "\n",
        "##Camadas Intermediárias\n",
        "Para o caso em que você só precisa de uma única saída de todo o processo, obter essa saída pode ser bastante simples, pois você pode facilmente pegar a saída produzida pela última célula RNN na sequência. Como esta saída final já passou por cálculos em todas as células anteriores, o contexto de todas as entradas anteriores foi capturado. Isso significa que o resultado final é realmente dependente de todos os cálculos e entradas anteriores.\n",
        "\n",
        "$hidden_t=F(hidden_{t−1},input_t)$\n",
        "\n",
        "Na primeira etapa, um estado oculto normalmente será semeado como uma matriz de zeros, de modo que possa ser alimentado na célula RNN junto com a primeira entrada na sequência. Nos RNNs mais simples, o estado oculto e os dados de entrada serão multiplicados por matrizes de peso inicializadas por meio de um esquema como Xavier ou Kaiming (você pode ler mais sobre este tópico aqui). O resultado dessas multiplicações será então passado por uma função de ativação (como uma função tanh) para introduzir a não linearidade.\n",
        "\n",
        "$hidden_t=tanh(weight_{hidden}∗hidden_{t−1}+weight_{input}∗input_t)$\n",
        "\n",
        "Além disso, se precisarmos de uma saída ao final de cada etapa de tempo, podemos passar o estado oculto que acabamos de produzir por meio de uma camada linear ou apenas multiplicá-lo por outra matriz de peso para obter a forma desejada do resultado.\n",
        "\n",
        "\n",
        "$output_t=weight_{output}∗hidden_t$\n",
        "\n",
        "O estado oculto que acabamos de produzir será então realimentado na célula RNN junto com a próxima entrada e este processo continua até que fiquemos sem entrada ou o modelo seja programado para parar de produzir saídas.\n",
        "\n",
        "##Training and Back-propagation\n",
        "Semelhante a outras formas de redes neurais, os modelos RNN precisam ser treinados para produzir saídas precisas e desejadas após a passagem de um conjunto de entradas.\n",
        "\n",
        "Durante o treinamento, para cada dado de treinamento, teremos um rótulo de verdade do terreno correspondente ou simplesmente colocaremos uma \"resposta correta\" que queremos que o modelo produza. Claro, nas primeiras vezes que passamos os dados de entrada pelo modelo, não obteremos saídas iguais a essas respostas corretas. No entanto, depois de receber esses resultados, o que faremos durante o treinamento é calcular a perda desse processo, que mede o quão longe o resultado do modelo está da resposta correta. Usando essa perda, podemos calcular o gradiente da função de perda para retropropagação.\n",
        "\n",
        "Com o gradiente que acabamos de obter, podemos atualizar os pesos no modelo de forma adequada para que cálculos futuros com os dados de entrada produzam resultados mais precisos. O peso aqui se refere às matrizes de peso que são multiplicadas com os dados de entrada e estados ocultos durante o passe para frente. Todo esse processo de cálculo dos gradientes e atualização dos pesos é chamado de retropropagação. Combinado com a passagem para a frente, a retropropagação é repetida continuamente, permitindo que o modelo se torne mais preciso com suas saídas a cada vez que os valores das matrizes de peso são modificados para selecionar os padrões dos dados.\n",
        "\n",
        "Embora possa parecer que cada célula RNN está usando um peso diferente, conforme mostrado nos gráficos, todos os pesos são na verdade os mesmos, pois a célula RNN está sendo essencialmente reutilizada ao longo do processo. Portanto, apenas os dados de entrada e o estado oculto transportado são exclusivos em cada etapa de tempo.\n",
        "\n",
        "##Implementação\n",
        "Nesta implementação, será construído um modelo que pode completar sua frase com base em alguns caracteres ou uma palavra usada como entrada.\n",
        "\n",
        "O modelo será alimentado com uma palavra e preverá qual será o próximo caractere da frase. Este processo se repetirá até que geremos uma frase do tamanho desejado.\n",
        "\n",
        "Para manter isso curto e simples, não usaremos nenhum conjunto de dados grande ou externo. Em vez disso, definiremos apenas algumas frases para ver como o modelo aprende com essas frases. O processo que essa implementação levará é o seguinte:\n",
        "\n",
        "\n",
        "\n",
        "1.   Criação do dicionário\n",
        "2.   Pré-processamento: padding e divisão entre entrada e rótulo\n",
        "3. Codificação _One-Hot_\n",
        "4. Definição do Modelo\n",
        "5. Treinamento do Modelo\n",
        "6. Avaliação do Modelo\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYnaZev7g0O-"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFBKq4ZDpgGx"
      },
      "source": [
        "Primeiro, definiremos as sentenças que queremos que nosso modelo produza quando alimentado com a primeira palavra ou os primeiros caracteres.\n",
        "\n",
        "Então, vamos criar um dicionário de todos os caracteres que temos nas frases e mapeá-los para um número inteiro. Isso nos permitirá converter nossos caracteres de entrada em seus respectivos inteiros (char2int) e vice-versa (int2char)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKaJg1w4pnid",
        "outputId": "af67f039-7efe-4af4-8198-e0cd3f93432e"
      },
      "source": [
        "text = ['oi como voce esta', 'eu estou bem', 'tenha um bom dia']\n",
        "\n",
        "# Join all the sentences together and extract the unique characters from the combined sentences\n",
        "chars = set(''.join(text))\n",
        "\n",
        "# Creating a dictionary that maps integers to the characters\n",
        "int2char = dict(enumerate(chars))\n",
        "\n",
        "# Creating another dictionary that maps characters to integers\n",
        "char2int = {char: ind for ind, char in int2char.items()}\n",
        "\n",
        "print(char2int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'s': 0, 'n': 1, 'v': 2, 'i': 3, 'b': 4, ' ': 5, 'e': 6, 't': 7, 'h': 8, 'm': 9, 'a': 10, 'd': 11, 'c': 12, 'u': 13, 'o': 14}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTPQ_u6up9i9"
      },
      "source": [
        "A seguir, as sentenças de entrada serão preenchidas para garantir que todas as sentenças tenham o mesmo tamanho. Embora os RNNs possam receber entradas de tamanhos variáveis, geralmente queremos alimentar os dados de treinamento em lotes para acelerar o processo de treinamento. Para usar os lotes para treinar em nossos dados, precisaremos garantir que cada sequência nos dados de entrada tenha o mesmo tamanho.\n",
        "\n",
        "Portanto, na maioria dos casos, o preenchimento pode ser feito preenchendo as sequências muito curtas com valores 0 e corte das sequências muito longas. Em nosso caso, encontraremos o comprimento da sequência mais longa e preencheremos o restante das frases com espaços em branco para corresponder a esse comprimento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BGtWR4Aq3VI",
        "outputId": "f6db0ca1-85ab-420c-b9b6-558d742351f4"
      },
      "source": [
        "maxlen = len(max(text, key=len))\n",
        "print(\"The longest string has {} characters\".format(maxlen))\n",
        "\n",
        "\n",
        "# Padding\n",
        "\n",
        "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of the sentence matches\n",
        "# the length of the longest sentence\n",
        "for i in range(len(text)):\n",
        "    while len(text[i])<maxlen:\n",
        "        text[i] += ' '"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The longest string has 17 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-7bO9zIrLk4"
      },
      "source": [
        "Como vamos prever o próximo caractere na sequência, a cada passo de tempo, teremos que dividir cada frase em:\n",
        "\n",
        "* Dados de entrada:\n",
        "O último caractere de entrada deve ser excluído, pois não precisa ser alimentado no modelo\n",
        "*Rótulo/alvo:\n",
        "Um caractere à frente dos dados de entrada, pois esta será a \"resposta correta\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40FANHZUr46h",
        "outputId": "033fa60b-ca07-49d7-f6a5-a1ac9fc19aef"
      },
      "source": [
        "# Creating lists that will hold our input and target sequences\n",
        "input_seq = []\n",
        "target_seq = []\n",
        "\n",
        "for i in range(len(text)):\n",
        "    # Remove last character for input sequence\n",
        "    input_seq.append(text[i][:-1])\n",
        "    \n",
        "    # Remove firsts character for target sequence\n",
        "    target_seq.append(text[i][1:])\n",
        "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Sequence: oi como voce est\n",
            "Target Sequence: i como voce esta\n",
            "Input Sequence: eu estou bem    \n",
            "Target Sequence: u estou bem     \n",
            "Input Sequence: tenha um bom dia\n",
            "Target Sequence: enha um bom dia \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biGU9RDYsEU8"
      },
      "source": [
        "Agora podemos converter nossas sequências de entrada e destino em sequências de inteiros em vez de caracteres, mapeando-os usando os dicionários que criamos acima. Isso nos permitirá codificar a quente nossa sequência de entrada posteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg6WLI4zsPXn"
      },
      "source": [
        "for i in range(len(text)):\n",
        "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
        "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpHQYBE3sO-T"
      },
      "source": [
        "Antes de codificar nossa sequência de entrada em vetores one-hot, definiremos 3 variáveis ​​principais:\n",
        "\n",
        "* dict_size: o número de caracteres únicos que temos em nosso texto. Isso determinará o tamanho de um vetor quente, pois cada caractere terá um índice atribuído nesse vetor\n",
        "\n",
        "* seq_len: o comprimento das sequências que estamos alimentando no modelo. Como padronizamos o comprimento de todas as nossas frases para serem iguais às frases mais longas, este valor será o comprimento máximo - 1, pois removemos a entrada do último caractere também\n",
        "\n",
        "* batch_size: o número de sentenças que definimos e que alimentaremos no modelo como um lote\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z_pI5Evsuu3",
        "outputId": "e7304a4d-2a24-466d-ea48-5bb466490238"
      },
      "source": [
        "dict_size = len(char2int)\n",
        "seq_len = maxlen - 1\n",
        "batch_size = len(text)\n",
        "\n",
        "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
        "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
        "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "    \n",
        "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
        "    for i in range(batch_size):\n",
        "        for u in range(seq_len):\n",
        "            features[i, u, sequence[i][u]] = 1\n",
        "    return features\n",
        "\n",
        "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
        "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n",
        "\n",
        "input_seq = torch.from_numpy(input_seq)\n",
        "target_seq = torch.Tensor(target_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape: (3, 16, 15) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ5KH9e0tnzZ"
      },
      "source": [
        "Para começar a construir nosso próprio modelo de rede neural, podemos definir uma classe que herda a classe base de PyTorch (nn.module) para todos os módulos de rede neural. Depois de fazer isso, podemos começar a definir algumas variáveis ​​e também as camadas para o nosso modelo sob o construtor. Para este modelo, usaremos apenas 1 camada de RNN seguida por uma camada totalmente conectada. A camada totalmente conectada será responsável por converter a saída RNN em nossa forma de saída desejada.\n",
        "\n",
        "Também teremos que definir a função forward pass em forward () como um método de classe. A ordem em que a função direta é executada sequencialmente, portanto, teremos que passar as entradas e o estado oculto inicializado com zero através da camada RNN primeiro, antes de passar as saídas RNN para a camada totalmente conectada. Observe que estamos usando as camadas que definimos no construtor.\n",
        "\n",
        "O último método que temos que definir é o método que chamamos anteriormente para inicializar o estado oculto - init_hidden (). Isso basicamente cria um tensor de zeros na forma de nossos estados ocultos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A0KJR7b2O1l"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        #Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        \n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4aP05Yg2dFC"
      },
      "source": [
        "Depois de definir o modelo acima, teremos que instanciar o modelo com os parâmetros relevantes e definir nossos hiperparâmetros também. Os hiperparâmetros que definimos abaixo são:\n",
        "\n",
        "* n_epochs: Number of Epochs -> Refere-se ao número de vezes que nosso modelo percorrerá todo o conjunto de dados de treinamento\n",
        "* lr: Taxa de aprendizagem -> Isso afeta a taxa em que nosso modelo atualiza os pesos nas células cada vez que a retropropogação é feita\n",
        "\n",
        "Uma taxa de aprendizagem menor significa que o modelo altera os valores do peso com uma magnitude menor\n",
        "Uma maior taxa de aprendizagem significa que os pesos são atualizados em uma extensão maior para cada etapa\n",
        "Semelhante a outras redes neurais, temos que definir o otimizador e a função de perda também. Usaremos CrossEntropyLoss, pois o resultado final é basicamente uma tarefa de classificação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8OvsCWt2rd5",
        "outputId": "1c899196-1c53-4575-e1bd-015ea0f90191"
      },
      "source": [
        "# Instantiate the model with hyperparameters\n",
        "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
        "print(model)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 100\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (rnn): RNN(15, 12, batch_first=True)\n",
            "  (fc): Linear(in_features=12, out_features=15, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB_35nYq2y8Y"
      },
      "source": [
        "Agora podemos começar nosso treinamento! Como temos apenas algumas frases, esse processo de treinamento é muito rápido. No entanto, conforme progredimos, conjuntos de dados maiores e modelos mais profundos significam que os dados de entrada são muito maiores e o número de parâmetros dentro do modelo que temos que calcular é muito maior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKeZjMR324S3",
        "outputId": "dc71091d-8fef-4eba-a964-caa9515d0815"
      },
      "source": [
        "# Training Run\n",
        "input_seq = input_seq.to(device)\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "    #input_seq = input_seq.to(device)\n",
        "    output, hidden = model(input_seq)\n",
        "    output = output.to(device)\n",
        "    target_seq = target_seq.to(device)\n",
        "    loss = criterion(output, target_seq.view(-1).long())\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "    \n",
        "    if epoch%10 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10/100............. Loss: 2.3268\n",
            "Epoch: 20/100............. Loss: 2.0647\n",
            "Epoch: 30/100............. Loss: 1.7181\n",
            "Epoch: 40/100............. Loss: 1.3383\n",
            "Epoch: 50/100............. Loss: 1.0026\n",
            "Epoch: 60/100............. Loss: 0.7197\n",
            "Epoch: 70/100............. Loss: 0.5005\n",
            "Epoch: 80/100............. Loss: 0.3463\n",
            "Epoch: 90/100............. Loss: 0.2427\n",
            "Epoch: 100/100............. Loss: 0.1841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bWEiiUh2-Lw"
      },
      "source": [
        "Vamos testar nosso modelo agora e ver que tipo de saída obteremos. Antes disso, vamos definir alguma função auxiliar para converter a saída do nosso modelo de volta para texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgJtxOxg3B6Q"
      },
      "source": [
        "def predict(model, character):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    character = np.array([[char2int[c] for c in character]])\n",
        "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
        "    character = torch.from_numpy(character)\n",
        "    character = character.to(device)\n",
        "    \n",
        "    out, hidden = model(character)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "    # Taking the class with the highest probability score from the output\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "    return int2char[char_ind], hidden\n",
        "\n",
        "\n",
        "def sample(model, out_len, start='oi'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    # First off, run through the starting characters\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, chars)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x5p6i-aO3SU4",
        "outputId": "4ed8faab-343b-4bb1-fc60-ef01239eb3ff"
      },
      "source": [
        "sample(model, 17, 'tenha')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tenha um bom dia '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}