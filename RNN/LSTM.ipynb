{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "LSTM with Convolutional Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/redes_neurais_esp/blob/main/RNN/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F1S973y9IId"
      },
      "source": [
        "#LSTM\n",
        "\n",
        "Embora os LSTMs sejam uma espécie de RNN e funcionem de maneira semelhante aos RNNs tradicionais, seu mecanismo de portas é o que o diferencia. Este recurso aborda o problema de “memória de curto prazo” dos RNNs. \n",
        "\n",
        "![](https://raw.githubusercontent.com/valmirf/redes_neurais_esp/main/RNN/FIG/rnnlstm.jpg)\n",
        "\n",
        "Como podemos ver na imagem acima, a diferença reside principalmente na capacidade do LSTM de preservar a memória de longo prazo. Isso é especialmente importante na maioria do Processamento de Linguagem Natural (PNL) ou séries temporais e tarefas sequenciais. Considere o exemplo em que temos uma rede gerando texto com base em alguma entrada que nos foi fornecida. No início do texto, menciona-se que o autor tem um “cachorro chamado Cliff”. Depois de algumas outras frases em que não há menção a um animal de estimação ou cachorro, o autor traz seu animal de estimação novamente, e o modelo deve gerar a próxima palavra para \"No entanto, Cliff, meu animal de estimação ____\" Como a palavra animal de estimação apareceu logo antes do espaço em branco, um RNN pode deduzir que a próxima palavra provavelmente será um animal que pode ser mantido como animal de estimação.\n",
        "![](https://raw.githubusercontent.com/valmirf/redes_neurais_esp/main/RNN/FIG/lstm.jpg)\n",
        "\n",
        "Porém, devido à memória de curto prazo, o RNN típico só conseguirá utilizar as informações contextuais do texto que apareceram nas últimas frases - o que não tem utilidade alguma. O RNN não tem ideia de qual animal o animal pode ser, pois as informações relevantes desde o início do texto já foram perdidas.\n",
        "\n",
        "Por outro lado, o LSTM pode reter as informações anteriores de que o autor tem um cão de estimação, e isso ajudará o modelo a escolher \"o cão\" quando se trata de gerar o texto naquele ponto devido às informações contextuais de um etapa de tempo anterior.\n",
        "\n",
        "![](https://raw.githubusercontent.com/valmirf/redes_neurais_esp/main/RNN/FIG/lstm2.jpg)\n",
        "\n",
        "##Implementação da LSTM\n",
        "Assim como os outros tipos de camadas, podemos instanciar uma camada LSTM e fornecer a ela os argumentos necessários. A documentação completa dos argumentos aceitos pode ser encontrada aqui. Neste exemplo, definiremos apenas a dimensão de entrada, dimensão oculta e o número de camadas.\n",
        "\n",
        "* Dimensão de entrada - representa o tamanho da entrada em cada etapa de tempo.\n",
        "* Dimensão oculta - representa o tamanho do estado oculto e o estado da célula em cada etapa de tempo.\n",
        "* Número de camadas - o número de camadas LSTM empilhadas umas sobre as outras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e28FHQv5AxEd"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "input_dim = 5\n",
        "hidden_dim = 10\n",
        "n_layers = 1\n",
        "\n",
        "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
        "print(lstm_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHj_IK2ZAyLG"
      },
      "source": [
        "Vamos criar alguns dados fictícios para ver como a camada recebe a entrada. Como nossa dimensão de entrada é 5, temos que criar um tensor da forma (1, 1, 5) que representa (tamanho do lote, comprimento da sequência, dimensão de entrada).\n",
        "\n",
        "\n",
        "Além disso, teremos que inicializar um estado oculto e um estado de célula para o LSTM, pois esta é a primeira célula. O estado oculto e o estado da célula são armazenados em uma tupla com o formato (oculto_estado, célula_estado)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxavEHkpA9Qy"
      },
      "source": [
        "batch_size = 1\n",
        "seq_len = 1\n",
        "\n",
        "inp = torch.randn(batch_size, seq_len, input_dim)\n",
        "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
        "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
        "hidden = (hidden_state, cell_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-zj64u-BMHK"
      },
      "source": [
        "Em seguida, alimentaremos a entrada e os estados ocultos e veremos o que obteremos disso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH-ObgNyBPDl"
      },
      "source": [
        "out, hidden = lstm_layer(inp, hidden)\n",
        "print(\"Output shape: \", out.shape)\n",
        "print(\"Hidden: \", hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WUy2eMwBTph"
      },
      "source": [
        "No processo acima, vimos como a célula LSTM processará a entrada e os estados ocultos em cada etapa de tempo. No entanto, na maioria dos casos, processaremos os dados de entrada em grandes sequências. O LSTM também pode receber sequências de comprimento variável e produzir uma saída em cada etapa de tempo. Vamos tentar alterar o comprimento da sequência desta vez."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIw9uhMEBYMC"
      },
      "source": [
        "seq_len = 3\n",
        "inp = torch.randn(batch_size, seq_len, input_dim)\n",
        "out, hidden = lstm_layer(inp, hidden)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkwDUVyWBbsb"
      },
      "source": [
        "Desta vez, a 2ª dimensão da saída é 3, indicando que havia 3 saídas fornecidas pelo LSTM. Isso corresponde ao comprimento de nossa sequência de entrada. Para os casos de uso em que precisaremos de uma saída em cada etapa de tempo (muitos para muitos), como Geração de Texto, a saída de cada etapa de tempo pode ser extraída diretamente da 2ª dimensão e alimentada em uma camada totalmente conectada. Para tarefas de classificação de texto (muitos para um), como Análise de sentimento, a última saída pode ser usada para alimentar um classificador."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO6yeHdQhMwE"
      },
      "source": [
        "## Análise de Sentimentos com LSTM\n",
        "Este notebook é adaptado do excelente material de Ben Trevett, disponível [aqui](https://github.com/bentrevett/pytorch-sentiment-analysis).\n",
        "\n",
        "Tradicionalmente, CNNS são utilizadas pra análise de imagens devido a operação de filtragem e extração de características das camadas convolucionais. \n",
        "\n",
        "Então, porque usar CNNS em textos? Da mesma forma que filtros de tamanho quadrados (Ex. `3x3`) extraem características de uma imagem, filtros de uma dimensão (Ex. `1x2`), pode olhar pra duas palavras no texto (Bi-gram). Nos modelos de CNN, filtros de diferentes tamanhos (`1xn`), são n-grams no texto.\n",
        "\n",
        "Nesta atividade, construiremos um modelo de aprendizado de máquina para detectar sentimentos (ou seja, detectar se uma frase é positiva ou negativa) usando PyTorch e TorchText. Isso será feito nas críticas de filmes, usando o [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dstn9n_FhMwI"
      },
      "source": [
        "## Preparação dos Dados\n",
        "Um dos principais conceitos do TorchText é o `Field` (campo). Eles definem como seus dados devem ser processados. Em nossa tarefa de classificação de sentimento, os dados consistem na avaliação da crítica e no sentimento sobre o filme, seja \"pos\" ou \"neg\".\n",
        "\n",
        "Os parâmetros de um `Field` especificam como os dados devem ser processados.\n",
        "\n",
        "Usamos o campo `TEXT` para definir como a revisão deve ser processada, e o campo` LABEL` para processar o sentimento.\n",
        "\n",
        "No campo `TEXT` tem` tokenize = 'spacy'` como argumento. Isso define que a \"tokenização\" (o ato de dividir a string em \"tokens\" discretos) deve ser feita usando o tokenizer [spaCy] (https://spacy.io). Se nenhum argumento `tokenize` for passado, o padrão é simplesmente dividir a string em espaços.\n",
        "\n",
        "`LABEL` é definido por um `LabelField`, um subconjunto especial da classe `Field` especificamente usado para lidar com rótulos. \n",
        "\n",
        "Para mais informações sobre `Fields`, acesse [aqui](https://github.com/pytorch/text/blob/master/torchtext/data/field.py).\n",
        "\n",
        "As sementes aleatórias são definidas para reprodutibilidade."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqZ1Fa-FhMwK"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy', batch_first = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq2ZVM_1hMwV"
      },
      "source": [
        "Outro recurso útil do TorchText é que ele oferece suporte para [conjuntos de dados](https://torchtext.readthedocs.io/en/latest/datasets.html) comuns usados ​​em processamento de linguagem natural (PNL).\n",
        "\n",
        "Por exemplo, o Torchtext disponibiliza os conjuntos de dados de várias atividades de PNL:\n",
        "\n",
        "\n",
        "* Análise de Sentimentos:\n",
        "  * SST\n",
        "  * IMDb\n",
        "* Classificação de Perguntas:\n",
        "  * TREC\n",
        "  * Entailment\n",
        "  * SNLI\n",
        "  * MultiNLI\n",
        "* Modelagem de Linguagem:\n",
        "  * WikiText-2\n",
        "  * WikiText103\n",
        "  * PennTreebank\n",
        "* Tradução:\n",
        "  * Multi30k\n",
        "  * IWSLT\n",
        "  * WMT14\n",
        "* _Tagging_ de Sequências:\n",
        "  * UDPOS\n",
        "  * CoNLL2000Chunking\n",
        "* Resposta de Perguntas: \n",
        "  * BABI20\n",
        "\n",
        "\n",
        "O código a seguir baixa automaticamente o conjunto de dados IMDb e divide-o em treino/teste utilizando `torchtext.datasets`. Ele processa os dados usando os `Campos` que foram definidos anteriormente. O conjunto de dados IMDb consiste em 50.000 críticas de filmes, cada uma marcada como sendo uma crítica positiva (`1`) ou negativa (`0`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtV6zZhdhMwV"
      },
      "source": [
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_qf9_13hMwd"
      },
      "source": [
        "Abaixo podemos ver a quantidade exemplos nos conjuntos de treino e no teste e um exemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ytF6PpxhMwf"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')\n",
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s9A-4kFW334"
      },
      "source": [
        "O conjunto de dados IMDb tem apenas divisões de treinamento/teste, portanto, se quisermos criar um conjunto de validação, temos que usar novamente o método `.split()`.\n",
        "\n",
        "Por padrão, ele divide 70/30, no entanto, ao passar um argumento `split_ratio`, pode-se mudar a proporção da divisão, ou seja, um` split_ratio` de 0.8 significaria que 80% dos exemplos compõem o conjunto de validação e 20% compõem o conjunto de teste.\n",
        "\n",
        "A semente aleatória no argumento `random_state`, é para garantir que obteremos a mesma divisão de validação/teste todas as vezes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMIn5M3OW4gW"
      },
      "source": [
        "#train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
        "valid_data, test_data = test_data.split(split_ratio=0.5, random_state = random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK1YoKCjW9gm"
      },
      "source": [
        "Abaixo podemos ver a quantidade exemplos nos conjuntos de treino, validação e no teste e um exemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsT8nzpuXAKg"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gapnpVCAXGSJ"
      },
      "source": [
        "Em seguida, iremos construir um _vocabulário_, que é uma tabela de busca, onde cada palavra única em seu conjunto de dados tem um _index_ correspondente (um identificador).\n",
        "\n",
        "Cada _index_ é usado para construir um vetor _one-hot_ para cada palavra. Um vetor one-hot é um vetor em que todos os elementos são 0, exceto um, que é 1, e a dimensionalidade é o número total de palavras únicas no vocabulário, comumente denotado por $V$.\n",
        "\n",
        "Considere a frase \"The cat sat on the mat\". O vocabulário (ou palavras únicas) nesta frase é (cat, mat, on, sat, the). Para representar cada palavra, será criado um vetor de zeros com comprimento igual ao vocabulário e, em seguida, colocaremos 1 no índice que corresponder à palavra. Essa abordagem é mostrada no diagrama a seguir.\n",
        "\n",
        "![](https://github.com/valmirf/redes_neurais_esp/blob/main/PyTorch/FIG/one-hot.png?raw=true)\n",
        "\n",
        "Porém, essa abordagem é ineficiente. Um vetor one-hot é escasso (ou seja, a maioria dos atributos é zero). Por exemplo, um vocabulário de 10.000, para codificar cada palavra, criaríamos um vetor em que 99,99% dos elementos são zero.\n",
        "\n",
        "O número de palavras exclusivas neste conjunto de treinamento é superior a 100.000, o que significa que os vetores one-hot terão mais de 100.000 dimensões! Isso tornará o treinamento lento e possivelmente não caberá na GPU, além de um vetor com muitos zeros.\n",
        "\n",
        "Existem duas maneiras de reduzir rapidamente o tamanho do vocabulário: pode-se apenas pegar as $n$ palavras mais comuns ou ignorar as palavras que aparecem menos de $m$ vezes. Faremos o primeiro, mantendo apenas as 25000 palavras principais.\n",
        "\n",
        "O que acontece com as palavras que aparecem nos exemplos, mas são cortadas do vocabulário? Elas são substuídas por um token especial _unk_ ou ` ` (vazio). Por exemplo, se a frase for \"Este filme é ótimo e eu adoro\", mas a palavra \"adoro\" não estiver no vocabulário, ela se torna: \"Este filme é ótimo e eu ` ` isso\".\n",
        "\n",
        "São adicionados dois especiais além dos que existem. Um é o token `<unk>` e o outro é um token `<pad>`.\n",
        "\n",
        "Quando inserimos frases no modelo, a entrada é de um _lote_ delas por vez, ou seja, mais de uma de cada vez, e todas as frases do lote precisam ter o mesmo tamanho. Portanto, para garantir que cada frase do lote tenha o mesmo tamanho, qualquer frase menor do que a mais longa do lote é preenchida como mostra a figura abaixo.\n",
        "\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment6.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-c5tHBkXywA"
      },
      "source": [
        "###Embeddings\n",
        "Em seguida, vem o uso de embeddings de palavras pré-treinadas como opção aos _embeddings_ de palavras inicializados aleatoriamente. Esses vetores são obtidos simplesmente especificando quais vetores queremos e passando-os como um argumento para `build_vocab`. TorchText lida com o download dos vetores e os associa às palavras corretas em nosso vocabulário.\n",
        "\n",
        "_Embeddings_ é uma estruturação de palavras numa representação \"aprendida\", em que palavras com significados semelhantes ou relacionadas têm uma codificação semelhante. A abordagem _embeddings_ para representar palavras e documentos pode ser considerada uma das principais descobertas do aprendizado profundo em problemas desafiadores de processamento de linguagem natural.\n",
        "\n",
        "Nos _embeddings_ de palavras, as palavras individuais são representadas como vetores de valor real em um espaço vetorial pré-definido. Cada palavra é mapeada para um vetor e os valores do vetor são aprendidos por algum algoritmo de criação de _embeddings_, como por exemplo, os algoritmos Word2Vec e Glove.\n",
        "\n",
        "![](https://github.com/valmirf/redes_neurais_esp/blob/main/PyTorch/FIG/embedding_example.png?raw=true)\n",
        "\n",
        "Acima está um diagrama para um embedding de uma palavra. Cada palavra é representada como um vetor quadridimensional de valores de ponto flutuante. Outra maneira de pensar em um embedding é como \"tabela de pesquisa\". Depois que esses pesos foram aprendidos, podemos codificar cada palavra procurando o vetor denso a que corresponde na tabela. A Figura abaixo representa um mapa de Embeddings de críticas de filmes.\n",
        "\n",
        "![](https://github.com/valmirf/redes_neurais_esp/blob/main/PyTorch/FIG/Embeddings.png?raw=true)\n",
        "\n",
        "\n",
        "Aqui, usaremos os vetores \"glove.6B.100d\". Glove é o algoritmo usado para calcular os vetores, clique [aqui](https://nlp.stanford.edu/projects/glove/) para mais informações. 6B indica que esses vetores foram treinados em 6 bilhões de tokens e 100d indica que esses vetores são 100 -dimensional.\n",
        "\n",
        "É possível encontrar outros vetores disponíveis [aqui](https://github.com/pytorch/text/blob/master/torchtext/vocab.py#L113).\n",
        "\n",
        "A teoria é que esses vetores pré-treinados já têm palavras com significado semântico semelhante, próximas umas das outras no espaço vetorial, por exemplo, \"agradável, \"aprazível\", \"prazeroso\", estão próximos. Isso dá à nossa camada de incorporação uma boa inicialização, pois ela não precisa aprender essas relações do zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyFPGtlQX6xB"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5Hmp5Y7ZOFY"
      },
      "source": [
        "A etapa final da preparação dos dados é a criação dos iteradores. \n",
        "\n",
        "Usaremos um `BucketIterator` que é um tipo especial de iterador que retornará um lote de exemplos em que cada exemplo tem um comprimento semelhante, minimizando a quantidade de preenchimento por exemplo.\n",
        "\n",
        "Para blocos de sequências preenchidas, todos os exemplos em um lote precisam ser classificados por seus comprimentos. Isso é tratado no iterador definindo `sort_within_batch = True`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OghvCv1TZjNL"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjoRfrSr_Q0"
      },
      "source": [
        "batch = next(iter(train_iterator))\n",
        "print('Sample input size: ', batch.text.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', batch.text)\n",
        "print()\n",
        "print('Sample label size: ', batch.label.size()) # batch_size\n",
        "print('Sample label: \\n', batch.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLjkQpj-hMwl"
      },
      "source": [
        "## Construção do Modelo\n",
        "\n",
        "A primeira etapa é converter as palavras em _embeddings_ de palavras. É assim que se transforma palavras em 2 dimensões, onde cada palavra é colocada ao longo de um eixo e os elementos do vetor aprendido é outra dimensão. Considere a representação bidimensional da frase incorporada abaixo:\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment9.png?raw=1)\n",
        "\n",
        "Pode-se usar um filtro $[n \\times emb_{dim}]$. o filtro passará por $n$ palavras sequenciais inteiramente, já que sua largura será a dimensão `emb_dim`. Considere a imagem abaixo, com os vetores de palavras representados em verde. Temos 4 palavras com _embeddings_ de 5 ​​dimensões, criando um tensor de \"imagem\" [4x5]. O filtro [2x5], em amarelo, passará por duas palavras por vez (ou seja, bi-grams). A saída deste filtro (mostrado em vermelho) será um único número real que é a soma ponderada de todos os elementos cobertos pelo filtro.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment12.png?raw=1)\n",
        "\n",
        "A ideia é que o maior valor da saída calculada é a característica \"mais importante\" para determinar o sentimento da crítica. A rede definirá através dos cálculos de pesos pelo algoritmo _backpropagation_ qual as características mais importantes. \n",
        "\n",
        "\n",
        "### Implementação\n",
        "As camadas convolucionais são implementadas com `nn.Conv2d`. O argumento `in_channels` é o número de \"canais\" de entrada da camada convolucional. O `out_channels` é o número de filtros e o `kernel_size` é o tamanho dos filtros. Cada um dos `kernel_size`s terá dimensão $[n \\times emb\\_dim]$ onde $n$ é o tamanho dos n-gramas e $emb_{dim}$ é a dimensão dos embeddings.\n",
        "\n",
        "A segunda dimensão da entrada em uma camada `nn.Conv2d` deve ser a dimensão do canal. O tamanho da saída da camada convolucional depende do tamanho da entrada, e diferentes lotes contêm sentenças de diferentes comprimentos. Sem a camada de _max-pooling_, a entrada para nossa camada linear dependeria do tamanho da frase de entrada (não o que queremos). Uma opção para retificar isso seria cortar/preencher todas as sentenças com o mesmo comprimento; no entanto, com a camada de _max-pooling_, sempre sabemos que a entrada para a camada linear será o número total de filtros. \n",
        "\n",
        "**Nota**: há uma exceção se a frase forem mais curta do que o maior filtro usado. Pois, a frase terá que ser preenchida com o comprimento do maior filtro. Nos dados do IMDb não há comentários de apenas 5 palavras, então não precisamos nos preocupar com isso.\n",
        "\n",
        "Neste modelo o token <pad> será ignorado. Isso ocorre porque queremos dizer explicitamente ao nosso modelo que os tokens de preenchimento <pad> são irrelevantes para determinar o sentimento de uma frase. Fazemos isso passando o índice do nosso token de pad como o argumento `padding_idx` para a camada `nn.Embedding`.\n",
        "\n",
        "Pra utilizar uma lista de filtros, utiliza-se `nn.ModuleList`, uma função usada para manter uma lista de PyTorch `nn.Module`. Então, no método `forward`, iteramos através da lista aplicando cada camada convolucional para obter uma lista de saídas convolucionais, que também é alimentada por meio da _max-pooling_ em uma lista, antes de concatenar e passar pelas camadas _dropout_ e linear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG9F5u_Qmo3a"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "class SentimentLSTMGRU(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5, model_type=\"LSTM\"):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.model_type = model_type\n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if model_type == \"GRU\":\n",
        "          self.lstmgru = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        else:\n",
        "          self.lstmgru = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                          dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstmgru(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        if self.model_type == \"GRU\":\n",
        "          hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                    weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        \n",
        "        \n",
        "        return hidden\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22N8MLvEhMw4"
      },
      "source": [
        "###Parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTRbTpF4bzfG"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.3\n",
        "\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "model_type = \"LSTM\"\n",
        "\n",
        "model = SentimentLSTMGRU(INPUT_DIM, OUTPUT_DIM, EMBEDDING_DIM, hidden_dim, n_layers, model_type=model_type)\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-6AjBrOhMw-"
      },
      "source": [
        "Método para verificar o número de parâmetros do nosso modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcFGNbsehMw-"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw_vaZj2hMxE"
      },
      "source": [
        "A seguir, os _embeddings_ pré-treinados são carregados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMLSZfGahMxF"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKWx632nhMxK"
      },
      "source": [
        "Em seguida, os pesos iniciais dos tokens desconhecidos e de preenchimento são inicializados com zero. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXDDsCiYhMxM"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LXWVc8yhMxQ"
      },
      "source": [
        "## Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBgf8nwjhMxR"
      },
      "source": [
        "São inicializados o otimizador, a função de perda (critério) e colocamos o modelo e o critério na GPU (se disponível)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLgLIxVYhMxT"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "#criterion = nn.BCEWithLogitsLoss()\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdxXWDSdhMxZ"
      },
      "source": [
        "Função pra calcular a acurácia:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXtHtzCIhMxa"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    num_correct = 0\n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(preds.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(y.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "    \n",
        "    acc = num_correct / len(preds)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwp8nZHIhMxg"
      },
      "source": [
        "###Função de treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eojlQS62hMxg"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, model_type=\"LSTM\"):\n",
        "    h = model.init_hidden(BATCH_SIZE)\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    count = 0\n",
        "    for batch in iterator:\n",
        "        if len(batch) == BATCH_SIZE: \n",
        "          count = count + 1\n",
        "          if model_type == \"GRU\":\n",
        "              h = h.data\n",
        "          else:\n",
        "              h = tuple([e.data for e in h])\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          predictions, h = model(batch.text, h)\n",
        "          loss = criterion(predictions.squeeze(), batch.label.float())\n",
        "          \n",
        "          acc = binary_accuracy(predictions, batch.label)\n",
        "          \n",
        "          loss.backward()\n",
        "          \n",
        "          optimizer.step()\n",
        "          \n",
        "          epoch_loss += loss.item()\n",
        "          epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / count, epoch_acc / count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0bqSQEkhMxl"
      },
      "source": [
        "###Função de Teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_grBjLrhMxl"
      },
      "source": [
        "def evaluate(model, iterator, criterion, model_type=\"LSTM\"):\n",
        "    val_h = model.init_hidden(BATCH_SIZE)\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        count = 0\n",
        "        for batch in iterator:\n",
        "            if len(batch) == BATCH_SIZE: \n",
        "              count = count + 1\n",
        "              if model_type == \"GRU\":\n",
        "                val_h = val_h.data\n",
        "              else:\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "              predictions,val_h = model(batch.text,val_h)\n",
        "              \n",
        "              loss = criterion(predictions, batch.label)\n",
        "              \n",
        "              acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "              epoch_loss += loss.item()\n",
        "              epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / count, epoch_acc / count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H0bdNVshMxp"
      },
      "source": [
        "Função pra calcular o tempo de execução das épocas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I26DpSZehMxp"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7B4-uLrhMxu"
      },
      "source": [
        "Execução de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "09FrvX35hMxv"
      },
      "source": [
        "N_EPOCHS = 2\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, model_type)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, model_type)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjRepwW9hMxz"
      },
      "source": [
        "Aqui os resultados do teste é mostrado pelo código abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QgS-OyThMxz"
      },
      "source": [
        "model.load_state_dict(torch.load('tut4-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion, model_type)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rbTppnuhMx5"
      },
      "source": [
        "## Entrada do usuário\n",
        "\n",
        "\n",
        "A função `predict_sentiment` é modificada para aceitar um argumento de comprimento mínimo do tamanho do maior filtro. Se a sentença de entrada tokenizada for menor que tokens `min_len`, são acrescentados tokens de preenchimento (`<pad>`) para torná-la tokens de tamanho `min_len`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQq2crFhhMx5"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence, min_len = 5):\n",
        "    model.eval()\n",
        "     # initialize hidden state\n",
        "    h = model.init_hidden(batch_size)\n",
        "    #h = tuple([sentence.data for each in h])\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    if len(tokenized) < min_len:\n",
        "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    out,h = model(tensor,h)\n",
        "    pred = torch.round(out.squeeze()) \n",
        "\n",
        "    # print custom response\n",
        "    if(pred.item()==1):\n",
        "        print(\"Crítica Positiva!\")\n",
        "    else:\n",
        "        print(\"Crítica Negativa.\")\n",
        "\n",
        "    return pred.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_TlQyADhMx_"
      },
      "source": [
        "Um exemplo de crítica negativa..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVToRAPAhMyB"
      },
      "source": [
        "predict_sentiment(model, \"This film is horrible\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzlH6Mk1hMyG"
      },
      "source": [
        "Um exemplo de crítica positiva..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-d_bx8LhMyG"
      },
      "source": [
        "predict_sentiment(model, \"This film is great\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}