{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Captioning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valmirf/redes_neurais_esp/blob/main/RNN/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga_FoP8YEwj5"
      },
      "source": [
        "#Imagem Captioning\n",
        "\n",
        "A tarefa de legendagem de imagens [Image Captioning](https://medium.com/analytics-vidhya/use-pytorch-to-create-an-image-captioning-model-with-cnn-and-seq2seq-lstm-and-train-on-google-e6563cb9f511) é uma das atividades que se tornaram mais impressionantes ultimamente. Essa atividade descreve objetos dentro de uma imagem. A partir de uma base de imagens com anotações, o algoritmo é capaz de descrever outras imagens. Para esta atividade, iremos utilizar a Base de Imagens COCO da Microsoft, pois ela contém as anotações necessárias pra treinar um algoritmo. \n",
        "\n",
        "Geralmente, um modelo de legendagem é uma combinação de duas arquiteturas separadas que são CNN (Redes Neurais Convolucionais) e RNN (Redes Neurais Recorrentes). Basicamente, o CNN é usado para gerar vetores de características a partir dos dados espaciais nas imagens e os vetores são alimentados através da camada linear totalmente conectada na arquitetura RNN, a fim de gerar os dados sequenciais ou sequência de palavras que no final geram a descrição de um imagem.\n",
        "\n",
        "##Base de Dados COCO\n",
        "O conjunto de dados COCO é um dos maiores conjuntos de dados de imagens publicamente disponíveis e destina-se a representar cenas realistas. Essas imagens vêm em uma variedade de formas com uma variedade de objetos e condições de ambientes e de iluminação que representam de perto o que você poderia obter se compilasse imagens de muitas câmeras diferentes em todo o mundo.\n",
        "\n",
        "Para explorar o conjunto de dados, você pode verificar o [site](https://cocodataset.org/#explore) da base de dados\n",
        "\n",
        "Cada imagem vem com 5 legendas diferentes produzidas por pessoas diferentes, portanto, cada legenda é um pouco (as vezes muito) diferente das outras legendas da mesma imagem. Aqui está um exemplo de um ponto de dados do conjunto de dados COCO.\n",
        "\n",
        "![](https://raw.githubusercontent.com/valmirf/redes_neurais_esp/main/RNN/FIG/coco.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEHyryqivS6i"
      },
      "source": [
        "## Download a Base de Dados: Anotações, legendas, imagens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0mg9Oqku4FY"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvml55V7vIBl"
      },
      "source": [
        "import os \n",
        "import sys\n",
        "from pycocotools.coco import COCO\n",
        "import urllib\n",
        "import zipfile "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY2yAgDovgLd"
      },
      "source": [
        "os.makedirs('opt' , exist_ok=True)\n",
        "os.chdir( '/content/opt' )\n",
        "!git clone 'https://github.com/cocodataset/cocoapi.git'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA6JaTnzvrhG"
      },
      "source": [
        "os.chdir('/content/opt/cocoapi')\n",
        "\n",
        "# Download the annotation : \n",
        "annotations_trainval2014 = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip'\n",
        "image_info_test2014 = 'http://images.cocodataset.org/annotations/image_info_test2014.zip'\n",
        "\n",
        "urllib.request.urlretrieve(annotations_trainval2014 , filename = 'annotations_trainval2014.zip' )\n",
        "urllib.request.urlretrieve(image_info_test2014 , filename= 'image_info_test2014.zip' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLfQ0PV-yfJ9"
      },
      "source": [
        "Extrai as anotações do arquivo ZIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8FJOO2mx2ft"
      },
      "source": [
        "with zipfile.ZipFile('annotations_trainval2014.zip' , 'r') as zip_ref:\n",
        "  zip_ref.extractall( '/content/opt/cocoapi'  )  \n",
        "\n",
        "try:\n",
        "  os.remove( 'annotations_trainval2014.zip' )\n",
        "  print('zip removed')\n",
        "except:\n",
        "  None\n",
        "\n",
        "with zipfile.ZipFile('image_info_test2014.zip' , 'r') as zip_ref:\n",
        "  zip_ref.extractall( '/content/opt/cocoapi'  )  \n",
        "\n",
        "try:\n",
        "  os.remove( 'image_info_test2014.zip' )\n",
        "  print('zip removed')\n",
        "except:\n",
        "  None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyKmXQkuz_zR"
      },
      "source": [
        "Inicializar e verificar os dados carregados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp6iuhP-yyI7"
      },
      "source": [
        "os.chdir('/content/opt/cocoapi/annotations')\n",
        "# initialize COCO API for instance annotations\n",
        "dataType = 'val2014'\n",
        "instances_annFile = 'instances_{}.json'.format(dataType)\n",
        "print(instances_annFile)\n",
        "coco = COCO(instances_annFile)\n",
        "\n",
        "# initialize COCO API for caption annotations\n",
        "captions_annFile = 'captions_{}.json'.format(dataType)\n",
        "coco_caps = COCO(captions_annFile)\n",
        "\n",
        "# get image ids \n",
        "ids = list(coco.anns.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOPxvivc0mZQ"
      },
      "source": [
        "Apresenta um exemplo de Imagem e Legendas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS6_GylyzosH"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import skimage.io as io \n",
        "import numpy as np \n",
        "%matplotlib inline "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXbkWYbX00sR"
      },
      "source": [
        "#Pick a random annotation id and display img of that annotation  :\n",
        "ann_id = np.random.choice( ids )\n",
        "img_id = coco.anns[ann_id]['image_id']\n",
        "img = coco.loadImgs( img_id )[0]\n",
        "url = img['coco_url']\n",
        "print(url)\n",
        "I = io.imread(url)\n",
        "plt.imshow(I)\n",
        "\n",
        "# Display captions for that annotation id :\n",
        "ann_ids = coco_caps.getAnnIds( img_id   )\n",
        "print('Number of annotations i.e captions for the image: ' , ann_ids)\n",
        "print()\n",
        "anns = coco_caps.loadAnns( ann_ids )\n",
        "coco_caps.showAnns(anns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ZkrGRi3X-l"
      },
      "source": [
        "Baixa as bases de dados de Treinamento, Validação e Teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZhRb--F1Ric"
      },
      "source": [
        "os.chdir('/content/opt/cocoapi')\n",
        "\n",
        "train2014 = 'http://images.cocodataset.org/zips/train2014.zip'\n",
        "test2014 = 'http://images.cocodataset.org/zips/test2014.zip'\n",
        "#val2014 = 'http://images.cocodataset.org/zips/val2014.zip'\n",
        "\n",
        "urllib.request.urlretrieve( train2014 , 'train2014' )\n",
        "urllib.request.urlretrieve( test2014 , 'test2014' )\n",
        "#urllib.request.urlretrieve( val2014 , 'val2014' )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-lJ6fizZx23"
      },
      "source": [
        "Descompacta as imagens baixadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QpNu1U8ZwvZ"
      },
      "source": [
        "os.chdir('/content/opt/cocoapi')\n",
        "with zipfile.ZipFile( 'train2014' , 'r' ) as zip_ref:\n",
        "  zip_ref.extractall( 'images' )\n",
        "\n",
        "try:\n",
        "  os.remove( 'train2014' )\n",
        "  print('zip removed')\n",
        "except:\n",
        "  None\n",
        "\n",
        "\n",
        "os.chdir('/content/opt/cocoapi')\n",
        "with zipfile.ZipFile( 'test2014' , 'r' ) as zip_ref:\n",
        "  zip_ref.extractall( 'images' )\n",
        "\n",
        "try:\n",
        "  os.remove( 'test2014' )\n",
        "  print('zip removed')\n",
        "except:\n",
        "  None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9fqDQO35KPG"
      },
      "source": [
        "## Passo 1: Manipulando a Base de Dados\n",
        "\n",
        "Nesta etapa, cria uma classe chamada Vocabulary pra manipular o texto contido nas imagens. \n",
        "\n",
        "\n",
        "No método __call__, a variável `word2idx` é um dicionário Python indexado por chaves string-valor (principalmente tokens obtidos das legendas de treinamento). Para cada chave, o valor correspondente é o número inteiro para o qual o token é mapeado na etapa de pré-processamento.\n",
        "\n",
        "O dicionário `word2idx` é criado repetindo as legendas no conjunto de dados de treinamento. Se um token aparecer não menos que o valor de `vocab_threshold` vezes no conjunto de treinamento, ele será adicionado como uma chave ao dicionário e atribuído a um número inteiro exclusivo correspondente. Observe que, em geral, valores menores para `vocab_threshold` geram um número maior de tokens no vocabulário. \n",
        "\n",
        "\n",
        "O valor `vocab_from_file` é utilizado para carregar um vocabulário de um arquivo. Observe que ao criar um novo carregador de dados, o vocabulário (`data_loader.dataset.vocab`) é salvo como um arquivo pickle na pasta do projeto, com o nome de arquivo `vocab.pkl`.\n",
        "\n",
        "Se você ainda está criando ou ajustanto um vocabulário, o valor do argumento deve ser `vocab_from_file=False`. Quando estiver satisfeito com o valor que escolheu para o argumento `vocab_threshold`, você só precisa executar o carregador de dados mais uma vez com o `vocab_threshold` escolhido para salvar o novo vocabulário no arquivo. Então, você pode definir 'vocab_from_file = True' para carregar o vocabulário do arquivo e acelerar a instanciação do _DataLoader_. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7eAPTW12RNE"
      },
      "source": [
        "# vocabulary.py -------------------------------------------------------------\n",
        "import nltk\n",
        "import pickle\n",
        "import os.path\n",
        "from pycocotools.coco import COCO\n",
        "from collections import Counter\n",
        "\n",
        "class Vocabulary(object):\n",
        "\n",
        "    def __init__(self,\n",
        "        vocab_threshold,\n",
        "        vocab_file='./vocab.pkl',\n",
        "        start_word=\"<start>\",\n",
        "        end_word=\"<end>\",\n",
        "        unk_word=\"<unk>\",\n",
        "        annotations_file='../cocoapi/annotations/captions_train2014.json',\n",
        "        vocab_from_file=False):\n",
        "        \"\"\"Initialize the vocabulary.\n",
        "        Args:\n",
        "          vocab_threshold: Minimum word count threshold.\n",
        "          vocab_file: File containing the vocabulary.\n",
        "          start_word: Special word denoting sentence start.\n",
        "          end_word: Special word denoting sentence end.\n",
        "          unk_word: Special word denoting unknown words.\n",
        "          annotations_file: Path for train annotation file.\n",
        "          vocab_from_file: If False, create vocab from scratch & override any existing vocab_file\n",
        "                           If True, load vocab from from existing vocab_file, if it exists\n",
        "        \"\"\"\n",
        "        self.vocab_threshold = vocab_threshold\n",
        "        self.vocab_file = vocab_file\n",
        "        self.start_word = start_word\n",
        "        self.end_word = end_word\n",
        "        self.unk_word = unk_word\n",
        "        self.annotations_file = annotations_file\n",
        "        self.vocab_from_file = vocab_from_file\n",
        "        self.get_vocab()\n",
        "\n",
        "    def get_vocab(self):\n",
        "        \"\"\"Load the vocabulary from file OR build the vocabulary from scratch.\"\"\"\n",
        "        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n",
        "            with open(self.vocab_file, 'rb') as f:\n",
        "                vocab = pickle.load(f)\n",
        "                self.word2idx = vocab.word2idx\n",
        "                self.idx2word = vocab.idx2word\n",
        "            print('Vocabulary successfully loaded from vocab.pkl file!')\n",
        "        else:\n",
        "            self.build_vocab()\n",
        "            with open(self.vocab_file, 'wb') as f:\n",
        "                pickle.dump(self, f)\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        \"\"\"Populate the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
        "        self.init_vocab()\n",
        "        self.add_word(self.start_word)\n",
        "        self.add_word(self.end_word)\n",
        "        self.add_word(self.unk_word)\n",
        "        self.add_captions()\n",
        "\n",
        "    def init_vocab(self):\n",
        "        \"\"\"Initialize the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        \"\"\"Add a token to the vocabulary.\"\"\"\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def add_captions(self):\n",
        "        \"\"\"Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold.\"\"\"\n",
        "        coco = COCO(self.annotations_file)\n",
        "        counter = Counter()\n",
        "        ids = coco.anns.keys()\n",
        "        for i, id in enumerate(ids):\n",
        "            caption = str(coco.anns[id]['caption'])\n",
        "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "            counter.update(tokens)\n",
        "\n",
        "            if i % 100000 == 0:\n",
        "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
        "\n",
        "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx[self.unk_word]\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFib3Yuq2D4i"
      },
      "source": [
        "### Cria o iterator\n",
        "A função `get_loader` recebe como entrada uma série de argumentos que podem ser explorados:\n",
        "\n",
        "1. `transform` - uma transformação de imagem que especifica como pré-processar as imagens e convertê-las em tensores PyTorch antes de usá-las como entrada para o codificador CNN. Por enquanto, você é encorajado a manter a transformação conforme fornecida em `transform_train`. \n",
        "2. `mode` - um de 'treinar' (carrega os dados de treinamento em lotes) ou 'teste' (para os dados de teste). Diremos que o carregador de dados está em modo de treinamento ou teste, respectivamente. Ao seguir as instruções neste notebook, mantenha o carregador de dados no modo de treinamento configurando `mode = 'train'`.\n",
        "2. `batch_size` - determina o tamanho do lote. Ao treinar o modelo, este é o número de pares de imagem-legenda usados ​​para corrigir os pesos do modelo em cada etapa de treinamento.\n",
        "3. `vocab_threshold` - o número total de vezes que uma palavra deve aparecer nas legendas de treinamento antes de ser usada como parte do vocabulário. Palavras com menos ocorrências de `vocab_threshold` nas legendas de treinamento são consideradas palavras desconhecidas.\n",
        "4. `vocab_from_file` - um Booleano que decide se deve carregar o vocabulário pré-existente.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUNLxxjD02kf"
      },
      "source": [
        "O método __getitem__ na classe CoCoDataset determina como um par de imagem-legenda é pré-processado antes de ser incorporado a um _batch_. \n",
        "\n",
        "Quando o _DataLoader_ está no modo de treinamento, este método começa primeiro obtendo o nome do arquivo (caminho) de uma imagem de treinamento e sua legenda (legenda) correspondente.\n",
        "\n",
        "Depois de carregar a imagem na pasta de treinamento com o caminho do nome, a imagem é pré-processada usando a mesma transformação (`transform_train`) que foi fornecida ao instanciar o carregador de dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiKijrXf72rw"
      },
      "source": [
        "# Data Loader ---------------------------------------------------------------------------------------------\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import json\n",
        "\n",
        "def get_loader(transform,\n",
        "               mode='train',\n",
        "               batch_size=1,\n",
        "               vocab_threshold=None,\n",
        "               vocab_file='./vocab.pkl',\n",
        "               start_word=\"<start>\",\n",
        "               end_word=\"<end>\",\n",
        "               unk_word=\"<unk>\",\n",
        "               vocab_from_file=True,\n",
        "               num_workers=0,\n",
        "               cocoapi_loc='/opt'):\n",
        "    \"\"\"Returns the data loader.\n",
        "    Args:\n",
        "      transform: Image transform.\n",
        "      mode: One of 'train' or 'test'.\n",
        "      batch_size: Batch size (if in testing mode, must have batch_size=1).\n",
        "      vocab_threshold: Minimum word count threshold.\n",
        "      vocab_file: File containing the vocabulary. \n",
        "      start_word: Special word denoting sentence start.\n",
        "      end_word: Special word denoting sentence end.\n",
        "      unk_word: Special word denoting unknown words.\n",
        "      vocab_from_file: If False, create vocab from scratch & override any existing vocab_file.\n",
        "                       If True, load vocab from from existing vocab_file, if it exists.\n",
        "      num_workers: Number of subprocesses to use for data loading \n",
        "      cocoapi_loc: The location of the folder containing the COCO API: https://github.com/cocodataset/cocoapi\n",
        "    \"\"\"\n",
        "    \n",
        "    assert mode in ['train', 'test'], \"mode must be one of 'train' or 'test'.\"\n",
        "    if vocab_from_file==False: assert mode=='train', \"To generate vocab from captions file, must be in training mode (mode='train').\"\n",
        "\n",
        "    # Based on mode (train, val, test), obtain img_folder and annotations_file.\n",
        "    if mode == 'train':\n",
        "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
        "        img_folder = os.path.join(cocoapi_loc, 'cocoapi/images/train2014/')\n",
        "        annotations_file = os.path.join(cocoapi_loc, 'cocoapi/annotations/captions_train2014.json')\n",
        "    if mode == 'test':\n",
        "        assert batch_size==1, \"Please change batch_size to 1 if testing your model.\"\n",
        "        assert os.path.exists(vocab_file), \"Must first generate vocab.pkl from training data.\"\n",
        "        assert vocab_from_file==True, \"Change vocab_from_file to True.\"\n",
        "        img_folder = os.path.join(cocoapi_loc, 'cocoapi/images/test2014/')\n",
        "        annotations_file = os.path.join(cocoapi_loc, 'cocoapi/annotations/image_info_test2014.json')\n",
        "\n",
        "    # COCO caption dataset.\n",
        "    dataset = CoCoDataset(transform=transform,\n",
        "                          mode=mode,\n",
        "                          batch_size=batch_size,\n",
        "                          vocab_threshold=vocab_threshold,\n",
        "                          vocab_file=vocab_file,\n",
        "                          start_word=start_word,\n",
        "                          end_word=end_word,\n",
        "                          unk_word=unk_word,\n",
        "                          annotations_file=annotations_file,\n",
        "                          vocab_from_file=vocab_from_file,\n",
        "                          img_folder=img_folder)\n",
        "\n",
        "    if mode == 'train':\n",
        "        # Randomly sample a caption length, and sample indices with that length.\n",
        "        indices = dataset.get_train_indices()\n",
        "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
        "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "        # data loader for COCO dataset.\n",
        "        data_loader = data.DataLoader(dataset=dataset, \n",
        "                                      num_workers=num_workers,\n",
        "                                      batch_sampler=data.sampler.BatchSampler(sampler=initial_sampler,\n",
        "                                                                              batch_size=dataset.batch_size,\n",
        "                                                                              drop_last=False))\n",
        "    else:\n",
        "        data_loader = data.DataLoader(dataset=dataset,\n",
        "                                      batch_size=dataset.batch_size,\n",
        "                                      shuffle=True,\n",
        "                                      num_workers=num_workers)\n",
        "\n",
        "    return data_loader\n",
        "\n",
        "class CoCoDataset(data.Dataset):\n",
        "    \n",
        "    def __init__(self, transform, mode, batch_size, vocab_threshold, vocab_file, start_word, \n",
        "        end_word, unk_word, annotations_file, vocab_from_file, img_folder):\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.batch_size = batch_size\n",
        "        self.vocab = Vocabulary(vocab_threshold, vocab_file, start_word,\n",
        "            end_word, unk_word, annotations_file, vocab_from_file)\n",
        "        self.img_folder = img_folder\n",
        "        if self.mode == 'train':            \n",
        "            self.coco = COCO(annotations_file)\n",
        "            self.ids = list(self.coco.anns.keys())\n",
        "            print('Obtaining caption lengths...')\n",
        "            all_tokens = [nltk.tokenize.word_tokenize(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in tqdm(np.arange(len(self.ids)))]\n",
        "            self.caption_lengths = [len(token) for token in all_tokens]\n",
        "        else:\n",
        "            test_info = json.loads(open(annotations_file).read())\n",
        "            self.paths = [item['file_name'] for item in test_info['images']]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # obtain image and caption if in training mode\n",
        "        if self.mode == 'train':\n",
        "            ann_id = self.ids[index]\n",
        "            caption = self.coco.anns[ann_id]['caption']\n",
        "            img_id = self.coco.anns[ann_id]['image_id']\n",
        "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
        "\n",
        "            # Convert image to tensor and pre-process using transform\n",
        "            image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
        "            image = self.transform(image)\n",
        "\n",
        "            # Convert caption to tensor of word ids.\n",
        "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
        "            caption = []\n",
        "            caption.append(self.vocab(self.vocab.start_word))\n",
        "            caption.extend([self.vocab(token) for token in tokens])\n",
        "            caption.append(self.vocab(self.vocab.end_word))\n",
        "            caption = torch.Tensor(caption).long()\n",
        "\n",
        "            # return pre-processed image and caption tensors\n",
        "            return image, caption\n",
        "\n",
        "        # obtain image if in test mode\n",
        "        else:\n",
        "            path = self.paths[index]\n",
        "\n",
        "            # Convert image to tensor and pre-process using transform\n",
        "            PIL_image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
        "            orig_image = np.array(PIL_image)\n",
        "            image = self.transform(PIL_image)\n",
        "\n",
        "            # return original image and pre-processed image tensor\n",
        "            return orig_image, image\n",
        "\n",
        "    def get_train_indices(self):\n",
        "        sel_length = np.random.choice(self.caption_lengths)\n",
        "        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n",
        "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
        "        return indices\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode == 'train':\n",
        "            return len(self.ids)\n",
        "        else:\n",
        "            return len(self.paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6yai64F2y6T"
      },
      "source": [
        "### Cria o DataLoader\n",
        "Cria o DataLoader que o PyTorch utiliza para iterar os dados nas etapas de treinamento e teste. Note que na etapa de treinamento vamos retornar o par (imagem, anotação). \n",
        "\n",
        "A entrada do codificador CNN pré-treinado aceita uma imagem de tamanho (224 x 224 x 3). Antes de fazer isso, aumentamos nossos dados de treinamento cortando aleatoriamente uma área da imagem depois de redimensionar a imagem para 256. Também usamos uma inversão horizontal para aumentar nossos dados com uma probabilidade de 0.5 e, em seguida, converte-se a imagem em um Tensor. Por fim, normalizamos a imagem pelos valores usados ​​para o conjunto de dados ImageNet.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8bGuKAz16XV"
      },
      "source": [
        "import sys\n",
        "from pycocotools.coco import COCO\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# Define a transform to pre-process the training images.\n",
        "transform_train = transforms.Compose([ \n",
        "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
        "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
        "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
        "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "# Set the minimum word count threshold.\n",
        "vocab_threshold = 8\n",
        "\n",
        "# Specify the batch size.\n",
        "batch_size = 64\n",
        "\n",
        "# Obtain the data loader.\n",
        "data_loader_train = get_loader(transform=transform_train,\n",
        "                         mode='train',\n",
        "                         batch_size=batch_size,\n",
        "                         vocab_threshold=vocab_threshold,\n",
        "                         vocab_from_file=False,\n",
        "                         cocoapi_loc = '/content/opt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uvEr04YXCo0"
      },
      "source": [
        "## Pré-processamento do texto\n",
        "Pré-processamento de legenda\n",
        "\n",
        "As legendas também precisam ser pré-processadas e preparadas para o treinamento. Neste exemplo, para gerar legendas, pretendemos criar um modelo que preveja o próximo token de uma frase de tokens anteriores, então transformamos a legenda associada a qualquer imagem em uma lista de palavras tokenizadas, antes de lançá-la em um tensor PyTorch que podemos usar para treinar a rede. Cada palavra é associada a um identificador criado a partir da base de dados de treinamento. Portanto, cada palavra da legenda terá um valor inteiro correspondente que pode ser encontrado num dicionário. As palavras neste dicionário são chamadas de vocabulário.\n",
        "\n",
        "\n",
        "As palavras especiais (`<start>`) e (`<end>`) são decididas ao instanciar o _DataLoader_ e são passadas como um parâmetro. Se o padrão for mantido, o inteiro 0 é sempre usado para marcar o início de uma legenda e o inteiro 1 para marcar o fim da legenda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1dH_gRXKxGE"
      },
      "source": [
        "import torch \n",
        "import numpy as np \n",
        "import torch.utils.data as data\n",
        "\n",
        "# Exploring the dataloader now :\n",
        "sample_caption = 'A person doing a trick on a rail while riding a skateboard.'\n",
        "sample_tokens = nltk.tokenize.word_tokenize( sample_caption.lower() )\n",
        "\n",
        "#Cria um vocabulário a partir das frases da base de treinamento\n",
        "sample_caption = []\n",
        "start_word  = data_loader_train.dataset.vocab.start_word\n",
        "end_word = data_loader_train.dataset.vocab.end_word\n",
        "sample_tokens.insert(0 , start_word)\n",
        "sample_tokens.append(end_word)\n",
        "sample_caption.extend( [ data_loader_train.dataset.vocab(token) for token in sample_tokens ] )\n",
        "\n",
        "\n",
        "sample_caption = torch.Tensor( sample_caption ).long()\n",
        "print('Find Below the Sample tokens and the idx values of those tokens in word2idx' , '\\n')\n",
        "print(sample_tokens) \n",
        "print(sample_caption )\n",
        "\n",
        "print('Find index values for words below \\n')\n",
        "print('Start idx {} , End idx {} , unknown idx {}'.format( 0,1,2 ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8OaFH9VMCmB"
      },
      "source": [
        "# Lets check word2idx in vocb \n",
        "print('First few vocab' , dict(list(data_loader_train.dataset.vocab.word2idx.items())[:10]))\n",
        "# Print the total number of keys in the word2idx dictionary.\n",
        "print('Total number of tokens in vocabulary:', len(data_loader_train.dataset.vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyxU5wWTPwtw"
      },
      "source": [
        "## Passo 2: Use o DataLoader para obter lotes\n",
        "As legendas no conjunto de dados variam muito em comprimento. É possível verificar isso examinando `data_loader.dataset.caption_lengths`, uma lista Python com uma entrada para cada legenda de treinamento (onde o valor armazena o comprimento da legenda correspondente).\n",
        "\n",
        "Na célula de código abaixo, usamos essa lista para imprimir o número total de legendas nos dados de treinamento com cada comprimento. Como você verá abaixo, a maioria das legendas tem tamanho 10. Da mesma forma, legendas muito curtas e muito longas são muito raras. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6HUbDqQPMyw"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "counter = Counter(data_loader_train.dataset.caption_lengths)\n",
        "lengths = sorted( counter.items() , key = lambda pair : pair[1] , reverse=True )\n",
        "for val,count in lengths:\n",
        "  print( 'value %2d  count %5d' %(val,count) )\n",
        "  if count < 10000: \n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJcMCb4ZXfSV"
      },
      "source": [
        "Para gerar lotes de dados de treinamento, vamos utilizar uma técnica descrita no [artigo](https://arxiv.org/pdf/1502.03044.pdf). Selecionamos um comprimento de legenda aleatoriamente (onde a probabilidade de que qualquer comprimento selecionado é proporcional ao número de legendas com aquele comprimento no conjunto de dados). Em seguida, recuperamos um lote de tamanho batch_size de pares imagem-legenda, onde todas as legendas têm o comprimento selecionado. \n",
        "\n",
        "Execute a célula de código abaixo para gerar um lote. O método `get_train_indices` na classe `CoCoDataset` primeiro faz a seleção de um comprimento de legenda e, em seguida, mostra os índices `batch_size` correspondentes com legendas desse comprimento. Esses índices são armazenados em `indices`.\n",
        "\n",
        "Esses índices são fornecidos ao _DataLoader_, que então é usado para recuperar os dados correspondentes. As imagens e legendas pré-processadas no lote são armazenadas em imagens e legendas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4MwT-xhRBY6"
      },
      "source": [
        "# Randomly sample a caption length, and sample indices with that length.\n",
        "indices = data_loader_train.dataset.get_train_indices()\n",
        "print('Sample Indices:' , indices )\n",
        "\n",
        "# Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
        "sampler = data.sampler.SubsetRandomSampler(  indices )\n",
        "data_loader_train.batch_sampler.sampler = sampler \n",
        "\n",
        "# obtain images, caption :\n",
        "images , captions = next(iter(data_loader_train))\n",
        "print(images.shape , captions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cym1ywj7eX6g"
      },
      "source": [
        "\n",
        "## Passo 3: Codificador CNN (_Encoder_)\n",
        "\n",
        "O codificador usa a arquitetura ResNet-50 pré-treinada (com a camada final totalmente conectada removida), para extrair características de um lote de imagens pré-processadas. A saída é então achatada em um vetor, antes de ser passada por uma camada `Linear` para transformar o vetor de características para o mesmo tamanho da palavra _embedding_.\n",
        "\n",
        "![](https://raw.githubusercontent.com/valmirf/redes_neurais_esp/main/RNN/FIG/encoder.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu6KyjYmfnlR"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        \n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)        \n",
        "        features = features.view(features.size(0), -1)        \n",
        "        features = self.embed(features)        \n",
        "        return features\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_7WcFWWf4fR"
      },
      "source": [
        "# specify dim of image embedding\n",
        "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "embed_size = 100 \n",
        "encoder = EncoderCNN( embed_size )\n",
        "encoder.to(device)\n",
        "images =  images.to(device) # images from step2 \n",
        "features = encoder(images)\n",
        "\n",
        "print(type(features) , features.shape , images.shape)\n",
        "assert( type(features) == torch.Tensor )  , 'Encoder output should be pytorch tensor'\n",
        "assert (features.shape[0] == batch_size) & (features.shape[1] == embed_size) , \"The shape of the encoder output is incorrect.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAWJt8yusTf3"
      },
      "source": [
        "## Step 4: Decodificador RNN (_Decoder_)\n",
        "O Decodificador é uma Rede Neural Recorrente. Abaixo, vamos usar uma Rede Neural LSTM. A LSTM será responsável por aprender a associar legendas com as características das imagens extraídas das imagens na etapa anterior. No código abaixo, `outputs` deve ser um tensor PyTorch com tamanho `[batch_size, captions.shape[1], vocab_size]`. Sua saída deve ser projetada de forma que `outputs[i, j, k]` contenha a previsão do modelo, indicando a probabilidade de o `j`-ésimo token da ` i`-ésima legenda no lote, ser o `k`-ésimo token no vocabulário. \n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/valmirf/redes_neurais_esp/main/RNN/FIG/decoder.png)\n",
        "\n",
        "A função `Predict` é usada para imprimir a legenda prevista do modelo para o vetor de características de uma imagem passada no argumento.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPvXgdlvzat5"
      },
      "source": [
        "import os \n",
        "import torch.utils.data as data \n",
        "import torch \n",
        "import math\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt \n",
        "% matplotlib inline\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super( DecoderRNN , self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size \n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "        self.word_embedding = nn.Embedding( self.vocab_size , self.embed_size )\n",
        "        self.lstm  = nn.LSTM(    input_size  =  self.embed_size , \n",
        "                             hidden_size = self.hidden_size,\n",
        "                             num_layers  = self.num_layers ,\n",
        "                             batch_first = True \n",
        "                             )\n",
        "        self.fc = nn.Linear( self.hidden_size , self.vocab_size  )\n",
        "        \n",
        "\n",
        "    def init_hidden( self, batch_size ):\n",
        "      return ( torch.zeros( self.num_layers , batch_size , self.hidden_size  ).to(device),\n",
        "      torch.zeros( self.num_layers , batch_size , self.hidden_size  ).to(device) )\n",
        "    \n",
        "    def forward(self, features, captions):            \n",
        "      captions = captions[:,:-1]      \n",
        "      self.batch_size = features.shape[0]\n",
        "      self.hidden = self.init_hidden( self.batch_size )\n",
        "      embeds = self.word_embedding( captions )\n",
        "      inputs = torch.cat( ( features.unsqueeze(dim=1) , embeds ) , dim =1  )      \n",
        "      lstm_out , self.hidden = self.lstm(inputs , self.hidden)      \n",
        "      outputs = self.fc( lstm_out )      \n",
        "      return outputs\n",
        "\n",
        "\n",
        "    def predict(self, inputs, max_len=20):        \n",
        "        final_output = []    \n",
        "        batch_size = inputs.shape[0] # batch_size is 1 at inference, inputs shape : (1, 1, embed_size)            \n",
        "        hidden = self.init_hidden(batch_size) # Get initial hidden state of the LSTM\n",
        "        while True:\n",
        "            lstm_out, hidden = self.lstm(inputs, hidden) # lstm_out shape : (1, 1, hidden_size) \n",
        "            outputs = self.fc(lstm_out)  # outputs shape : (1, vocab_size)\n",
        "            outputs = outputs.squeeze(1) \n",
        "            _, max_idx = torch.max(outputs, dim=1) # predict the most likely next word, max_indice shape : (1)            \n",
        "            final_output.append(max_idx.cpu().numpy()[0].item()) # storing the word predicted                        \n",
        "            if (max_idx == 1):\n",
        "                # We predicted the <end> word, so there is no further prediction to do\n",
        "                break            \n",
        "            ## Prepare to embed the last predicted word to be the new input of the lstm\n",
        "            inputs = self.word_embedding(max_idx) # inputs shape : (1, embed_size)\n",
        "            inputs = inputs.unsqueeze(1)    # inputs shape : (1, 1, embed_size)                     \n",
        "        return final_output  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qKA8QrdFjg"
      },
      "source": [
        "### Execução do Encoder e Decoder\n",
        "\n",
        "Temos dois componentes do modelo, o codificador e decodificador. Eles são treinados em conjunto, passando a saída do codificador, que é a rede neural convolucional, para o decodificador, que, por sua vez, é a rede neural recorrente conforme visto na figura abaixo. \n",
        "\n",
        "\n",
        "![](https://github.com/valmirf/redes_neurais_esp/blob/main/RNN/FIG/encoder-decoder.png?raw=true)\n",
        "\n",
        "Nesta etapa do notebook, você poderá personalizar o treinamento do modelo CNN-RNN, especificando hiperparâmetros e configurando outras opções importantes para o procedimento de treinamento. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jjOVmc82zl8"
      },
      "source": [
        "hidden_size = 256\n",
        "num_layers = 1 \n",
        "num_epochs = 1\n",
        "print_every = 200\n",
        "save_every = 1 \n",
        "vocab_size = len(data_loader_train.dataset.vocab)\n",
        "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Número total de passos de treinamento por época.\n",
        "total_step = math.ceil( len(data_loader_train.dataset.caption_lengths) / data_loader_train.batch_sampler.batch_size   )\n",
        "\n",
        "\n",
        "decoder = DecoderRNN(  embed_size , hidden_size, vocab_size ,num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.001\n",
        "#Especifica quais parâmetros da rede serão aprendidos\n",
        "all_params = list(decoder.parameters())  + list( encoder.embed.parameters() )\n",
        "optimizer = torch.optim.Adam( params  = all_params , lr = lr  )\n",
        "\n",
        "\n",
        "model_save_path = '/content/opt/cocoapi/RNN_LSTM/image_caption/CVND---Image-Captioning-Project/checkpoint'\n",
        "os.makedirs( model_save_path , exist_ok=True)\n",
        "\n",
        "# Save the params needed to created the model :\n",
        "decoder_input_params = {'embed_size' : embed_size , \n",
        "                'hidden_size' : hidden_size , \n",
        "                'num_layers' : num_layers,\n",
        "                'lr' : lr ,\n",
        "                'vocab_size' : vocab_size\n",
        "                }\n",
        "\n",
        "with open( os.path.join(model_save_path , 'decoder_input_params_27_11_2020.pickle'), 'wb') as handle:\n",
        "    pickle.dump(decoder_input_params, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRP1LVcoBAsv"
      },
      "source": [
        "import sys \n",
        "for e in range(num_epochs):\n",
        "  for step in range(total_step):\n",
        "    indices = data_loader_train.dataset.get_train_indices()\n",
        "    new_sampler = data.sampler.SubsetRandomSampler( indices )\n",
        "    data_loader_train.batch_sampler.sampler = new_sampler    \n",
        "    images,captions = next(iter(data_loader_train))    \n",
        "    images , captions = images.to(device) , captions.to(device)\n",
        "    encoder , decoder = encoder.to(device) , decoder.to(device)\n",
        "    encoder.zero_grad()    \n",
        "    decoder.zero_grad()\n",
        "    features = encoder(images)\n",
        "    output = decoder( features , captions )    \n",
        "    loss = criterion( output.view(-1, vocab_size) , captions.view(-1) )\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    stat_vals = 'Epochs [%d/%d] Step [%d/%d] Loss [%.4f] ' %( e+1,num_epochs,step,total_step,loss.item() )\n",
        "    if step % print_every == 0 :\n",
        "      print(stat_vals)\n",
        "      sys.stdout.flush()\n",
        "    if e % save_every == 0:\n",
        "      torch.save( encoder.state_dict() ,  os.path.join( model_save_path , 'encoderdata_{}.pkl'.format(e+1) ) )\n",
        "      torch.save( decoder.state_dict() ,  os.path.join( model_save_path , 'decoderdata_{}.pkl'.format(e+1) ) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q4CCgWF5xAz"
      },
      "source": [
        "Carrega o arquivo da rede salvo (_checkpoint_):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmfVe7pcCiYv"
      },
      "source": [
        "model_save_path = '/content/opt/cocoapi/RNN_LSTM/image_caption/CVND---Image-Captioning-Project/checkpoint'\n",
        "os.makedirs( model_save_path , exist_ok=True)\n",
        "\n",
        "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with open(  os.path.join(model_save_path , 'decoder_input_params_27_11_2020.pickle'), 'rb') as handle:\n",
        "    decoder_input_params = pickle.load(handle)\n",
        "\n",
        "embed_size = decoder_input_params['embed_size']\n",
        "hidden_size= decoder_input_params['hidden_size']\n",
        "vocab_size = decoder_input_params['vocab_size']\n",
        "num_layers = decoder_input_params['num_layers']\n",
        "\n",
        "encoder = EncoderCNN( embed_size )\n",
        "encoder.load_state_dict(   torch.load(  os.path.join( model_save_path , 'encoderdata_{}.pkl'.format(1) )   ) )\n",
        "\n",
        "decoder = DecoderRNN( embed_size , hidden_size , vocab_size , num_layers )\n",
        "decoder.load_state_dict( torch.load(   os.path.join( model_save_path , 'decoderdata_{}.pkl'.format(1) )   ) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeUg7D9D6Dnm"
      },
      "source": [
        "Cria o _Dataloader_ pra base de teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6BkposiQXEI"
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define a transform to pre-process the training images.\n",
        "transform_test = transforms.Compose([ \n",
        "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
        "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
        "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
        "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "# Obtain the data loader.\n",
        "data_loader_test = get_loader(transform=transform_test,\n",
        "                         mode='test',                         \n",
        "                         cocoapi_loc = '/content/opt')\n",
        "\n",
        "data_iter = iter(data_loader_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXQ2rzJYz4J6"
      },
      "source": [
        "def get_sentences( original_img, all_predictions ):\n",
        "  sentence = ' '\n",
        "  plt.imshow(original_img.squeeze())\n",
        "  return sentence.join([data_loader_test.dataset.vocab.idx2word[idx] for idx in all_predictions[1:-1] ]  )\n",
        "\n",
        "encoder.to(device) \n",
        "decoder.to(device)\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "original_img , processed_img  = next( data_iter )\n",
        "\n",
        "features  = encoder(processed_img.to(device)   ).unsqueeze(1)\n",
        "final_output = decoder.predict( features  , max_len=20)\n",
        "get_sentences(original_img, final_output)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}